Log Structured File System: Disk Organization Charmi Shah Department of Computer Science Engineering Oakland University Rochester Hills, Michigan USA charmishah@oakland.edu Abstract
According to previous research, log-structured file sys- tem (LFS) has the potential to significantly outperform typi- cal UNIX FFS in terms of write performance, recovery time, and file creation and deletion rates. Through this paper we are trying a simple implementation of a log-structured file system with focus on disk organization along with I/O mem- ory mapping which deal with large and complex data struc- ture on the disk. Here buffer cache will be handled by the host operating system memory management.
1. Introductio A research group at UC Berkeley led by Professor John Ousterhout and graduate student Mendel Rosenblum devel- oped a new file system known as the log-structured file sys- tem. Their motivation behind this new file system devel- opment was based on the following observations: 1) Sys- tem memories are growing rapidly 2) There is a large gap between random I/O performance and sequential I/O per- formance 3) Existing file systems perform poorly on many common workloads and 4) File systems are not RAID- aware.[1]
Log-structured is a disk storage management system, which is based on the assumption that files are cached in main memory and that increasing memory size will make the cache more effective with satisfying read request. A log-structured file system writes all modifications to disk se- quentially in a log-like structure, thereby speeding up both writing and crash recovery of the files. On disk, the log structure contains indexing information so the files can be read back from the log efficiently. In comparison to some other file system which uses log based system as temporary storage option, log-structured file systems stores data per-
Riya Singh
my Department of Computer Science Engineering
Oakland University Rochester Hills, Michigan USA riyasingh@oakland.edu
manently in log format on the disk, which is structured for such usage.[1]
Log-structured file system uses full potential of the disk array even if the file sizes are small, apart from log- structured file systems excellent ability to use full disk bandwidth it has other benefits as well, like Fast Recov- ery: Sequential nature of the log based system allows much faster crash recovery: current Unix file systems typically must scan the entire disk to restore consistency after a crash, but a log-structured file system need only examine the most recent portion of the log and Versioning: log-structured file systems keeps the older version intact, so in case of if the older version needs to be made available to user it could be achieved by saving it in slower mass storage section on disk.[2]
In recent years, CPU speed have improved by leaps and bounds, simultaneously the memory size also have in- creased but probably not as fast as the CPU speed. Contrary to this, disk speed has not improved as such a pace. This shift will open up new CPU-intensive applications develop- ment, but suggests that new I/O techniques will be needed to keep up the I/O from being a performance bottleneck.[5]
2. Problems and challenges
Modern file systems struggle to adapt to the technology and workloads of the 1990s due to two basic issues. First, they distribute data over the disk in a way that results in an excessive number of quick accesses. The Berkeley Unix fast file system (Unix FFS), for instance, performs an ex- cellent job at ordering the files sequentially on the disk, but it physically splits the different files. Additionally, a file’s characteristics (”inode”) and directory entry carrying the file’s name are distinct from the file’s contents. In UNIX FFS, its takes five I/O’s such as directory data, directory at- tributes, files data and two file attributes in order for writing

the new file, which in turn leads to only 5 percentage of time is spent in utilization of disk potential, while most of effort is spent behind seeking.[1]
The second issue with modern file systems is that they frequently write synchronously, requiring an application to wait for the write to finish before proceeding rather than handling the write in the background. Whereas, file system information structures like directories and inodes are writ- ten synchronously by UNIX FFS and file data blocks are written asynchronously by UNIX FFS. For the synchronous writes of small multiple file when coupled with disk per- formance it tend to make harder for the application to take advantage of the CPU performance as they would domi- nated by the synchronous writing of files at the same time. They also prevent the file cache from being used as a write buffer.[1]
To increase file I/O performance, several systems have adopted file caching. The goal of file caching is to save recently used disk blocks in main memory, so that further disk transfers for the same information can be avoided. File caching significantly lowers disk I/O performance because of the Locality in file access patterns. Nonetheless, file caching results in two main issues. 1) Future disk traffic will be increasingly dominated by writes, with most of the written data will remain in the file cache until it is over- written or erased. Disk I/O is mostly performed as a safety measure in case the cache contents are lost. 2) With only 3 percentage of the available raw disk bandwidth may be used if files are only 3 to 4 bytes long and two transfers are needed for each file. The only way to increase disk band- width by drastically lowering the number of seeks per file along accompanied with reduced file size.[5]
3. Log-structured file system Model
An ideal file system prioritizes write performance and tries to use the sequential bandwidth of the disk. Addition- ally, it will function effectively under typical workloads that often alter the metadata structure on disk in addition to writ- ing data.
Rosenblum and Ousterhout developed a brand-new form of file system called LFS or Log-Structured File System. When writing to the disk, LFS first buffers all changes which also includes metadata and kept together into an in- memory segment. When the segment is full, it is written to the disk in a long pass that is sequential to an empty area of the disk. LFS always writes segments to vacant slots in- stead of overwriting existing data. The huge segments allow for effective utilization of the disk and optimal file system efficiency.[1]
3.1 DISK organization :
Here we are trying to divide the disk into several seg- ments and superblocks each segment consists of a bunch of blocks and segment tables.
Figure 1. log-structured disk layout [6]
Above figure Fig 2 is a log-structured disk layout. log- structured allows fast restart from crash with 1) locating unused segments to write 2) last and next pointer in sum- mary allows recovery 3) Imap pointer helps finding inodes 4) Time stamp allows locating last successful checkpoint.
3.1.1 SEGMENT
The first issue with a log-structured file system is how to convert all file system state modifications into a sequence of sequential disk writes. Assume we are writing a data block d to a file; let’s say d is written at address A0. How- ever, when a user writes a data block, additional information which is metadata that has to be updated and be changed. Additionally, we set aside some room for the inode table, which contains a variety of on-disk inodes. To point to the data block d, we write the file’s inode to disk. The core of LFS is the simple concept of writing each update to the disk one at a time. As a result, we may issue many contiguous writes (or one massive write) to the drive-in order to get high write performance. Simply writing to the disk in se- quential order is not sufficient to reach peak performance.So a big in-memory segment serves as a buffer for all writes (data and metadata) (typically of some MB). There are sev- eral data blocks and inodes inside a segment. It is written to the append-only log when it is full. Specifically, each sec- tion is made up of: inodes, inode map, segment summary that contains the pointer to the next summary of next seg- ment and helps to identify for file block: the inode number and relative block number.[1]
The lay out of the log-structured file system, which shows storage been divided into segments regardless of the block type, for example inodes, data blocks, and direct blocks.
The disk are divided into segments and only one seg- ments can be active at one time in log-structured file sys- tem. Every individual segment has a header and it is called a summary block. This block contain a pointer to next sum- mary block and links to the long chain. This long chain that log-structured file system treat as a linear log. The segments are not joined with each other on the disk and because of
 
 Figure 2. This figure shows the layout of the log- structured file system.[1]
this larger segment size(between 384Kb and 1Mb) are rec- ommended.
LFS does this through a time-honored method called write buffering. LFS maintains track of changes in mem- ory prior to writing to the disk; when it has enough updates, it writes them all at once to the disk and making effective use of the disk possible. The name segment denotes the sub- stantial amount of updates that LFS writes all at once. Even though this segment is overused in computer systems, here it simply refers to the biggest chunk that LFS employs to aggregate writes. As a result, while writing to the disk, LFS buffers updates in a segment in memory before publishing the segment in its entirety to the disk. These writes will be effective if the section is big enough.
The following is an illustration of how LFS buffers two sets of updates into a single tiny segment; actual segments are bigger (a few MB). The first update involves writing four blocks to file j, while the second involves adding one block to file k. The full section of seven blocks is then simulta- neously committed to the disk by LFS. The arrangement of these blocks is as follows.
Figure 3.
3.1.2 BUFFERING TIME
Consequently, the question of how many updates LFS should buffer before writing to disk arises. The disk itself determines the answer, particularly how high the position- ing overhead is in relation to the transfer rate.
Consider the scenario where positioning, such as rotation and search overheads, occur before each write and require around T position seconds. Assume further that R MB/s is the disk transfer rate. On a disk like that, how much should LFS buffer before writing?
The best way to approach this is to think of the position- ing cost as a set overhead that you pay each time you write. What volume of writing is required to find a way to reduce that cost? The better (naturally) and closer you get to reach- ing its peak bandwidth, the more you write. We can assume of getting a specific response that we are writing D MB. The positioning time Tp plus the transfer time D (D/R) equals the time required to write down this chunk of data (T), or T = Tp + D /R.
Thus, the effective rate of writing (Reff), which is simply the quantity of data written divided by the total length of time to write it, is given by the formula Reff = D /T = D /(Tp+ D /R).
Here, we aim to narrow the gap between the effective rate and the R rate. A typical F maybe 0.9, or 90 percent of the R rate, since 0 F 1. Specifically, we want the effective rate to be some fraction of F of the R rate. This indicates that in mathematical terms, we need Reff = F* R. [1]
3.1.3 Inode
On the disk, inodes are exist in a static form. Disk inode contains owners information, file type, file access permis- sion, access time, number of links, file size and array of disk blocks. The in-core inode is an additional field of the disk inode. It contains, status of inodes, the device number, inonde number, points to inode and reference count.
There is a noticeable difference between a buffer header and an in-core inode is reference count. When a process al- locates inode the reference count will increase. If reference count is 0 than it will be in a free list and if reference count is greater than 0 that means that processes are accessing in- odes. When processes are accessing inodes it means inodes are in hash queue.
To access the inode we use iget algorithm, that will al- locate an in-core copy of an inode. If inodes are found in free list it will allocate an inode and reads the disk copy into the in-core inode. As we mentioned above the in-core con- tains some fields, means this in-core copy knows the inode number and device number.
According to number of inodes fit on one disk block, this algorithm will calculates the logical block number. Here is the algorithm for logical block number :
block number=((inode number - 1) / number of inodes per block) + start block of inode list
 
3.1.4 How to find Inode
We have to notice how to discover an inode in a standard UNIX file system to better understand how we find inodes in LFS. Finding inodes is simple in a normal file system like FFS or even the previous UNIX file system since they are arranged in an array and put in fixed places on a disk. The ancient UNIX file system, for instance, stores each inode at a set location on the disk. Therefore, given an inode number and the start address, you may identify a specific inode by calculating its precise disk address by multiplying the inode number by the size of an inode and adding that value to the on-disk array start address; this is known as array-based in- dexing is quick and simple. Because FFS divides the inode table into sections and sets a group of inodes within each cylinder group, finding an inode given an inode number is just a little more difficult.
Therefore, one has to be aware of the start addresses and size of each block of inodes. Calculations after that are comparable and simple. It is more challenging in LFS be- cause we were able to disperse the inodes throughout the whole disk. To address this, the creators of LFS created a data structure known as the inode map that adds a layer of indirection between inode numbers and the inodes (imap). An inode number is entered into the imap structure, which outputs the disk address of the inode’s most current ver- sion. So, it seems sensible that it would frequently be im- plemented as a straightforward array with 4 bytes (a disk pointer) for each item. The imap is always updated with an inode’s new position whenever one is written to disk.
So, the imap must be maintained persistent (that is, writ- ten to disk); and doing this enables LFS to remember the lo- cations of inodes despite crashes and function as intended. Here we know the location of imap might reside on a set area of the disk. Because of frequently changing of inode, performance would suffer if modifications to file structures weren’t followed by writes to the imap (i.e., there would be more disk seeks, between each update and the fixed location of the imap). Instead, LFS writes new data immediately ad- jacent to where it is writing portions of the inode map. LFS puts the new data block, its inode, and a portion of the inode map all together onto the disk when appending a data block to a file k, as seen below:
Figure 4.
The portion of the imap array seen above is contained in the block labeled imap, which informs LFS that the inode k is at disk address A1 and that its data block D is located at
address A0 through the inode.
Consider the size of the inode map if there were 100,000
inodes.
100000 * 4 = 400000 bytes, or 390 KB, is the size of the
inode map.
Inode maps can therefore be entirely cached in memory
due to their modest size. There is a duplicate of the inode map in each segment on the disk.[1]
3.1.5 Where to put the inode map on disk?
A specific area of the disk may be used to mount an inode map. But because it is updated regularly, we must also up- date the inode map each time a segment is altered. Thus, there would be more disk seeks, which might have an in- fluence on performance. Please be aware that we sync seg- ments to disk before they are filled with updated inodes or data. Since the segments are only partially changed, updat- ing the inode map with each partial write might affect speed. Therefore, we maintain the inode map for each segment. [1]
3.1.6 Find the inode-map
The address of the most recent inode-map is stored by LFS in a predefined location on the hard drive known as the checkpoint region (CR). Every time we create a new sec- tion, the CR needs to be updated. As a result, CR needs to be changed only seldom.[1]
Figure 5.
Figure 6.
The overall name of the file system is a log-structured file system since each segment in this case has a pointer to the following segment. Beginning with the CR, we may locate the initial segment written to the disk, follow the chain to determine what modifications were made to the file system, and ultimately locate the most recent segment.[1]
3.1.7 Segment Cleaning
Segment cleaning part is critical in log-structured file sys- tem. log-structured file system will work fast only if there is more segments are available is disk. Once the files get modified the use of segments drops over a time. Almost all
   
the segments are not found totally empty. Segment clean- ing is also tricky. Normal log use is dominated by writes and minimises seeks. Cleaning is dominated by reads, and requires many seeks.
The process of copying live data out of a segment is called a segment cleaning. We have used a basic approach to clean the segment. We are trying to first read a number of segment into memory and identifying the current data after this we write the current data into smaller number of clean segment. Once this process is done, the segments will be marked as clean and can be used for new data. In our im- plementation we use 0 and 1, 0 is for segment is clean and 1 is for segment is not clean.
As part of segment cleaning it must be possible to iden- tify which blocks of each segment are live, so that they can be written out again. It must also be possible to identify the file to which each block belongs and the position of the block within the file; this information is needed in order to update the file’s inode to point to the new location of the block. [1]
The summary block identifies each piece of information that is written in the segment; for example, for each file data block the summary block contains the file number and block number for the block. Segments can contain multiple segment summary blocks when more than one log write is needed to fill the segment.
3.1.8 LFS : Crash Recovery
Log-structured file systems can recover from unexpected crashes by using the most recent checkpoint, which leaves them in a consistent state and removes any later-written data. Additionally, we introduce a function to recover nearly all of the data that was written after a checkpoint.
Until now, LFS provides two options for regaining con- trol after an unplanned accident. The easier option is to ignore any data that was written after the last checkpoint, which is assured to be in a perfectly consistent state. In addition, we offer a roll-forward tool that allows you to re- cover as much data as you can, even if it was added after a checkpoint.As long as the segments checksums are accu- rate, this utility begins at the most recent checkpoint and proceeds along the chain of segments that have been writ- ten since then. By looking at the segment summary and file information records, all entities in each of these segments are found, and the necessary metadata is updated. The tool updates the relevant indirect block or inode whenever a data block is read.The inode table is updated to point to this copy of the inode if an inode is found. The segment usage table needs to be changed in both scenarios.
The utility must additionally handle inode and directory consistency. The link counter of an inode may not match the number of directory entries if a crash happens while
only a portion of a directory action has been written to the disk, or such an item may refer to an inode that doesn’t exist or even the erroneous inode.The fundamental issue is that the majority of directory actions touch multiple inodes, thus either all changes must be recovered during roll for- ward, or none at all. In order to combat the issue, BSD- LFS marks any partial segments that have unfinished direc- tory activities and refuses to conduct roll-forward on them unless another segment comes after it and finishes the op- erations. For each directory operation, Sprite-LFS add a record to the log. Together, these documents make up a di- rectory operation journal.Both file systems ensure that the relevant journal item comes before any affected inode or directory block in the log. Although this method makes roll-forward more challenging to construct, it permits the recovery of more data and places relatively few restrictions on the directory operations’ implementation, allowing for simpler and quicker execution.
Pointers to all of the on-disk imap components are con- tained in the on-disk checkpoint region.
1.In the log-structured file system, checkpoint regions are updated every 30 seconds.
2.After a crash, the checkpoint log-structured file system can be used to recreate the in-memory map.
3.To ensure that it will be automatically updated, the log- structured file system creates two copies of checkpoint re- gions in two separate areas of the disk.
4.LFS switches which place gets the current checkpoint.
5.When a checkpoint is interrupted, LFS writes a times- tamp,the checkpoint is only valid if both timestamps match and pointers to the imap fragments are provided.
6.This method is effective, but we lose the final 30 sec- onds of data writing.
4
Implementation Progress
Here we are trying to implement a simple log-structured file system that focuses on disk organization along with I/O memory mapping which deal with large and complex data structure on the disk and buffer cache will be handled by the host operating system memory management.
The sizes and types of several disk data structures, such as blocks, block identifiers, and inodes, are currently be- ing implemented. So, in this implementation, we are defin- ing two block addresses: blockid and blockadd. blockid is a logical address that identifies the logical position of the block in the file system.
Consider the scenario where a block is the 42nd direct block pointed to by the second indirect block inside of file 37. In this scenario, file 37’s indirect block would be located inside layers [0.....3]. Layers [0..3] would contain the block in the inode in this scenario, and Layers [5] would store the

offset of the 42nd direct block in the indirect block. Lay- ers [0..depth-1] are the only ones used, and the depth deter- mines how far to traverse the tree.
Block add, is a one which specifies a segment number and block within that segment, is how we store a physical address on disk.
Portion indirect block, part data make up an INode. The first direct pointers are to data, and the following indirect links are to individual indirect blocks.An indirect block, which includes the addresses of further blocks, an inode, or data are all examples of blocks (which has some of both).
In further implementation, we’ll aim to set up memory mapped files where segment data structures can be main- tained as well as organize blocks on the physical disk. Ad- ditionally, we are working to provide a basic user interface that will preserve Inode structure.
5. Feasible new Method or Solution
Feasible solution to solve this modern file system solu- tion is log structure file system. Aggregating tiny random writes into a single large page and inserting it in a cyclic log to copy them to the ”ultimate destination” later, when the demand on the disk subsystem is lower, is the concept that eventually gave rise to the concept of ”log-structuring”. File data blocks, attributes, index blocks, directories, and almost all the additional information needed to maintain the file system are all included in the information sent to disk during the write process.[1]
A log-structured file system transforms the numerous tiny synchronous random writes of standard file systems into huge concurrent sequential transfers that may utilize about 100 percentage of the raw disk capacity for work- loads that comprise of several small files[1] like fast failure recovery, fast snapshots and remarkable findings in terms of the randomized writes pattern.
But unlike the cache-logging method, it only utilizes one disk version of the data. Log-structured file systems might potentially boost disk bandwidth utilization along with cer- tain other additional attractive qualities.[5]
6. Future Research Plan
Overall, using a log-structured file system is a smart con- cept, but you should take caution as it is not suitable for all jobs and workloads. Log-structure file system comes with following problem that need to be resolved in near future, such as.
1) Sequential reading performance is unpredictable. It might not be as effective as traditional file systems (in case the data is written in the next sector). Additionally, it might be as terrible as random reading as opposed to sequential
reading. Performance essentially relies on the kind and size of the workload at the time of writing. 2) The need for “Garbage Collection”. 3) The requirement for a lot of empty space. 4) Manage large and complex data structures on disk. 5) Work with flash memory in common formats, such as SSD and NVMe, has been inadequate.
References
[1] Mendel Rosenblum. 1992. ”The design and imple- mentation of a log-structured file system.” Ph.D. Dis- sertation. University of California at Berkeley, USA.
[2] H. Gwak, Y. Kang and D. Shin, ”Reducing garbage collection overhead of log-structured file systems with GC journaling,” 2015 International Symposium on Consumer Electronics (ISCE), 2015
[3] Jonggyu Park, Dong Hyun Kang, and Young Ik Eom. 2016. File Defragmentation Scheme for a Log- Structured File System.In Proceedings of the 7th ACM SIGOPS Asia-Pacific Workshop on Systems (APSys ’16). Association for Computing Machinery, New York, NY, USA, Article 19.
[4] Martin Jambor, Tomas Hruby, Jan Taus, Kuba Kr- chak, and Viliam Holub. 2007. Implementation of a Linux log-structured file system with a garbage col- lector. SIGOPS Oper. Syst. Rev. 41, 1 (January 2007)
[5] F. Douglis and J. Ousterhout, ”Log-structured file systems,” Digest of Papers. COMPCON Spring 89. Thirty-Fourth IEEE Computer Society International Conference: Intellectual Leverage
